# RustRAGLab (RRL): A Rust Framework for RAG-Aware Fine-Tuning and Evaluation

[![Rust](https://img.shields.io/badge/rust-1.70%2B-orange.svg)](https://www.rust-lang.org/)
[![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/)
[![React](https://img.shields.io/badge/react-18.2-61dafb.svg)](https://reactjs.org/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

---

## ğŸ¯ Motivation

Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by connecting them to external knowledge sources, improving accuracy and grounding. However, there is currently **no Rust-native framework** that supports the entire RAG workflow from ingestion to evaluation.

Existing tools such as **LangChain**, **LlamaIndex**, and **Haystack** are Python-based, while Rust developers must piece together fragmented crates (e.g., `hnsw_rs`, `tantivy`, `candle`) without a standardized architecture.

**RustRAGLab (RRL)** fills this gap by providing a **unified, performant, and safe Rust framework** for building and evaluating RAG systems.

### Why Rust?
Rust provides **memory safety**, **low runtime overhead**, and **predictable concurrency**, making it ideal for building high-performance retrieval and training pipelines without Python dependencies.

---

## ğŸš€ Objective

Design and implement an **end-to-end Rust-native framework** that integrates retrieval, adapter fine-tuning, and evaluation for RAG systemsâ€”offering both developer APIs and a CLI tool (`rrl`) for streamlined workflows.

**NEW:** Complete web interface for training, evaluation, and RAG workflows with live monitoring.

---

## âœ¨ Key Features

### 1. Data & Chunking
- âœ… **Document Loader** â€” Plain text (`.txt`) files supported
- âœ… **Chunking Strategies** â€” Fixed-size and overlapping chunking methods
- âœ… **Preprocessing Pipeline** â€” Tokenization, stopword filtering, sentence segmentation
- âœ… **CLI Command:** `rrl ingest --input ./docs --output ./output/chunks`

### 2. Embeddings & Model Support
- âœ… **Trait-based Embedder Interface** for modular backend integration
- âœ… **Backend** â€” **Candle** (Rust-native ML framework)
- âœ… **Hardware Acceleration** â€” Support for **CUDA (NVIDIA)** and **Metal (Apple)** GPUs
- âœ… **Encoder Models** â€” BERT, RoBERTa, BGE, E5, DistilBERT, ALBERT, DeBERTa
- âœ… **Decoder Models (LLM)** â€” **Qwen2**, **LLaMA**, **Mistral** for text generation
- âœ… **Persistent Cache** â€” SQLite storage with versioning
- âœ… **CLI Command:** `rrl embed --input ./data/chunks.json --output ./data/embeddings.safetensors`

### 3. Indexing & Retrieval
- âœ… **Dense Retrieval** â€” HNSW via `hnsw_rs`
- âœ… **Sparse Retrieval** â€” BM25 via `tantivy`
- âœ… **Hybrid Retriever** â€” Weighted fusion of dense and sparse signals
- âœ… **Evaluation Metrics** â€” Recall@k, Mean Reciprocal Rank (MRR)
- âœ… **CLI Command:** `rrl query --index ./index --query "What is RAG?"`

### 4. Fine-Tuning (RAG-Aware)
- âœ… **LoRA / QLoRA / DoRA Fine-Tuning** using **Candle** (CUDA + Metal backends)
- âœ… **Multi-Architecture Support:**
  - **Encoder Models:** BERT, RoBERTa, BGE, E5, DistilBERT, ALBERT, DeBERTa
  - **Decoder Models:** **Qwen2**, **LLaMA**, **Mistral** (for generation fine-tuning)
- âœ… **Multi-Adapter Support** â€” Train and switch between task-specific adapters
- âœ… **Training Optimizations:**
  - Flash Attention (3.5x speedup)
  - Mixed Precision Training (2x speedup)
  - Gradient Checkpointing
  - Distributed Training (4x with 4 GPUs)
- âœ… **Memory Efficiency** â€” Train 7B-70B models on consumer GPUs with QLoRA
- âœ… **Grounding-Aware Loss** â€” Aligns model attention with retrieved chunks
- âœ… **CLI Command:** `rrl train --data ./data/train.jsonl --model BAAI/bge-base-en-v1.5`

### 5. Evaluation
- âœ… **Retrieval Metrics** â€” Recall@k, Mean Reciprocal Rank (MRR)
- âœ… **Generation Metrics** â€” Perplexity, Exact Match (EM), F1, ROUGE-L
- âœ… **Attribution Metrics** â€” Support fraction and citation precision/recall
- âœ… **CLI Command:** `rrl eval-mc --data ./data/test.json --model bert-base-uncased`

### 6. Developer Interfaces

**CLI Commands:**
```bash
rrl ingest    # Load and chunk documents
rrl embed     # Compute embeddings and build indexes
rrl index     # Build retrieval indexes (HNSW, BM25)
rrl query     # Query retrieval indexes
rrl train     # Fine-tune LoRA adapters (encoder/decoder models)
rrl eval      # Evaluate retrieval performance
rrl eval-mc   # Evaluate multiple-choice accuracy
rrl rag       # Run full RAG pipeline with LLM generation (Qwen2/LLaMA/Mistral)
rrl infer     # Run inference on a model
rrl serve     # Launch API server
```

**Rust API / SDK:**
- âœ… Modular traits: `Embedder`, `Retriever`, `Trainer`, `Evaluator`
- âœ… Integration with other Rust-based ML systems
- âœ… Type-safe configuration and error handling

### 7. Web Interface (Primary UI)

**Complete React-based UI with live monitoring** â€” the primary way to interact with RRL:
- âœ… **Live Training Dashboard** â€” Real-time metrics, charts, logs via WebSocket
- âœ… **Model Browser** â€” Explore and configure model architectures
- âœ… **Training Launcher** â€” Interactive job configuration and management
- âœ… **Evaluation Dashboard** â€” Test model performance with detailed metrics
- âœ… **Inference Playground** â€” Interactive model testing environment
- âœ… **RAG Workflow** â€” 4-step pipeline (Ingest â†’ Embed â†’ Index â†’ Query)
- âœ… **Data Upload** â€” Drag-and-drop dataset management

> **Note:** The Web UI is the recommended interface. Terminal UI (ratatui) development has been transitioned to focus on the Web UI.

**Access:** `http://localhost:5173` (after running `npm run dev`)

### 8. Server & API

**FastAPI Backend:**
- âœ… **REST API** â€” Complete API for all RRL operations
- âœ… **WebSocket** â€” Live training updates and streaming
- âœ… **File Upload** â€” Dataset upload with progress tracking
- âœ… **Job Management** â€” Start, stop, monitor training jobs
- âœ… **Model Serving** â€” Inference endpoints for trained models

**Access:** `http://localhost:8000/docs` (API documentation)

---

## ğŸ“ Project Structure

```
rrl/
â”œâ”€â”€ src/                        # ğŸ¦€ Rust source code
â”‚   â”œâ”€â”€ cli/                    # âœ… Command-line interface
â”‚   â”œâ”€â”€ cuda/                   # âœ… CUDA kernels for GPU acceleration
â”‚   â”œâ”€â”€ data/                   # âœ… Dataset handling
â”‚   â”œâ”€â”€ embedding/              # âœ… Embedding generation
â”‚   â”œâ”€â”€ evaluation/             # âœ… Model evaluation metrics
â”‚   â”œâ”€â”€ rag/                    # âœ… RAG system implementation
â”‚   â”œâ”€â”€ retrieval/              # âœ… Vector search and indexing
â”‚   â”œâ”€â”€ server/                 # âœ… Server utilities
â”‚   â”œâ”€â”€ training/               # âœ… Training system
â”‚   â”‚   â”œâ”€â”€ dataset.rs          # Dataset loading
â”‚   â”‚   â”œâ”€â”€ device.rs           # Device management (CPU/CUDA/Metal)
â”‚   â”‚   â”œâ”€â”€ evaluation.rs       # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ optimizer.rs        # AdamW optimizer
â”‚   â”‚   â”œâ”€â”€ tokenizer.rs        # Tokenization
â”‚   â”‚   â”œâ”€â”€ trainer.rs          # Training loop
â”‚   â”‚   â””â”€â”€ models/             # 10+ model architectures
â”‚   â”œâ”€â”€ tui/                    # âœ… Terminal UI
â”‚   â”œâ”€â”€ utils/                  # âœ… Utility functions
â”‚   â”œâ”€â”€ lib.rs                  # Library exports
â”‚   â””â”€â”€ main.rs                 # CLI entry point
â”œâ”€â”€ server.py                   # ğŸ†• FastAPI backend
â”œâ”€â”€ ui/                         # ğŸ†• React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/              # UI page components
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.jsx   # Live training monitor
â”‚   â”‚   â”‚   â”œâ”€â”€ Training.jsx    # Training launcher
â”‚   â”‚   â”‚   â”œâ”€â”€ Models.jsx      # Model browser
â”‚   â”‚   â”‚   â”œâ”€â”€ Evaluation.jsx  # Evaluation dashboard
â”‚   â”‚   â”‚   â”œâ”€â”€ Inference.jsx   # Inference playground
â”‚   â”‚   â”‚   â”œâ”€â”€ RAG.jsx         # RAG workflow
â”‚   â”‚   â”‚   â””â”€â”€ DataUpload.jsx  # Dataset uploader
â”‚   â”‚   â”œâ”€â”€ App.jsx             # Main application
â”‚   â”‚   â”œâ”€â”€ api.js              # API client
â”‚   â”‚   â””â”€â”€ main.jsx            # Entry point
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ test-docs/                  # ğŸ†• Sample documents for testing
â”‚   â”œâ”€â”€ ml.txt
â”‚   â”œâ”€â”€ rag.txt
â”‚   â””â”€â”€ rust.txt
â”œâ”€â”€ Cargo.toml                  # Rust dependencies
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ Proposal.md                 # Original project proposal
â”œâ”€â”€ CODE_STANDARDS.md           # ğŸ†• Code formatting guidelines
â””â”€â”€ TASK_MANAGEMENT.md          # ğŸ†• Development workflow
```

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Rust 1.70+
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Python 3.8+
python --version

# Node.js 16+
node --version
```

### Installation

```bash
# 1. Clone repository
git clone https://github.com/kevinlin29/ECE1724.git
cd ECE1724/rrl

# 2. Build Rust backend (choose one based on your hardware)

# CPU-only build (training features enabled)
cargo build --release --features training

# CUDA GPU build (NVIDIA GPUs - recommended for training)
cargo build --release --features cuda

# Metal GPU build (Apple Silicon)
cargo build --release --features metal

# 3. Install Python dependencies (for Web UI backend)
pip install fastapi uvicorn websockets python-multipart

# 4. Install UI dependencies
cd ui
npm install
```

### Build Feature Flags

| Feature | Description | Use Case |
|---------|-------------|----------|
| `training` | Enables fine-tuning capabilities | CPU-only training |
| `cuda` | CUDA GPU acceleration + training | NVIDIA GPU training |
| `metal` | Metal GPU acceleration + training | Apple Silicon |

### Run the Platform

**Terminal 1 - Backend API:**
```bash
python server.py
# Runs on http://localhost:8000
```

**Terminal 2 - Frontend UI:**
```bash
cd ui
npm install
npm run dev
# Runs on http://localhost:5173
```

**Open browser:** http://localhost:5173

---

## ğŸ“– Usage Examples

### 1. RAG Workflow (Web UI - Recommended)

1. **Open RAG Interface:** http://localhost:5173/rag

2. **Tab 1: Ingest Documents**
   ```
   Input Directory: ./test-docs
   Chunk Size: 512
   Chunk Overlap: 50
   â†’ Click "Ingest Documents"
   ```

3. **Tab 2: Generate Embeddings**
   ```
   Model: BAAI/bge-base-en-v1.5
   Batch Size: 32
   â†’ Click "Generate Embeddings"
   ```

4. **Tab 3: Build Index**
   ```
   Index Type: HNSW (Fast)
   â†’ Click "Build Index"
   ```

5. **Tab 4: Query**
   ```
   Query: "What is machine learning?"
   Top K: 5
   â†’ Click "Search"
   â†’ View ranked results with scores
   ```

### 2. RAG Workflow (CLI)

```bash
# Step 1: Ingest documents
rrl ingest --input ./test-docs --output ./output/chunks

# Step 2: Generate embeddings
rrl embed \
  --input ./output/chunks \
  --output ./output/embeddings \
  --model BAAI/bge-base-en-v1.5

# Step 3: Build indexes
rrl index \
  --chunks ./output/chunks \
  --embeddings ./output/embeddings \
  --output ./output/indexes \
  --model BAAI/bge-base-en-v1.5 \
  --index-type both  # builds both HNSW and BM25

# Step 4: Query (retrieval only)
rrl query \
  --index ./output/indexes \
  --query "What is RAG?" \
  --top-k 5 \
  --retriever hybrid
```

### 3. RAG with LLM Generation (CLI)

Use the `rrl rag` command for full retrieval-augmented generation with **Qwen2** or **LLaMA**:

```bash
# Single query with Qwen2 (default)
rrl rag \
  --index ./output/indexes \
  --query "What is machine learning?" \
  --generator Qwen/Qwen2.5-0.5B \
  --embedder bert-base-uncased \
  --top-k 5 \
  --device auto

# Interactive mode with LLaMA
rrl rag \
  --index ./output/indexes \
  --generator meta-llama/Llama-2-7b-hf \
  --embedder BAAI/bge-base-en-v1.5 \
  --retriever hybrid \
  --temperature 0.7 \
  --max-tokens 512

# With fine-tuned checkpoints
rrl rag \
  --index ./output/indexes \
  --query "How do I make pasta?" \
  --generator Qwen/Qwen2.5-0.5B \
  --generator-checkpoint ./outputs/final/lora_weights.safetensors \
  --embedder bert-base-uncased \
  --embedder-checkpoint ./outputs/embedder/lora_weights.safetensors \
  --format json
```

**RAG Command Options:**
| Option | Description | Default |
|--------|-------------|---------|
| `--generator` | LLM for generation (Qwen2, LLaMA, Mistral) | `Qwen/Qwen2.5-0.5B` |
| `--embedder` | Encoder model for retrieval | `bert-base-uncased` |
| `--retriever` | Retriever type: dense, sparse, hybrid | `hybrid` |
| `--top-k` | Number of documents to retrieve | `5` |
| `--temperature` | Sampling temperature (0 = greedy) | `0.7` |
| `--max-tokens` | Maximum tokens to generate | `512` |
| `--template` | Prompt template: default, concise, detailed | `default` |
| `--format` | Output format: text, json | `text` |
| `--dtype` | Model dtype: f32, f16, bf16 | `f16` |

### 4. Train a Model (Web UI)

1. **Open Training Interface:** http://localhost:5173/training
2. Select model (e.g., `BAAI/bge-base-en-v1.5`)
3. Upload dataset or specify path
4. Configure hyperparameters:
   - Epochs: 3
   - Batch Size: 32
   - Learning Rate: 5e-5
   - LoRA Rank: 16
5. Click "Start Training"
6. Watch live metrics and logs in real-time

### 5. Train a Model (CLI)

```bash
rrl train \
  --data ./data/train.jsonl \
  --output ./outputs \
  --model BAAI/bge-base-en-v1.5 \
  --epochs 3 \
  --batch-size 32 \
  --lora-rank 16 \
  --learning-rate 5e-5
```

### 6. Evaluate Model

**Web UI:**
1. Go to http://localhost:5173/evaluation
2. Select model and checkpoint
3. Upload test data
4. Click "Run Evaluation"
5. View accuracy and MRR metrics

**CLI:**
```bash
rrl eval-mc \
  --data ./data/test.json \
  --model BAAI/bge-base-en-v1.5 \
  --checkpoint ./outputs/checkpoint-500/lora_weights.safetensors
```

---

## ğŸ—“ï¸ Development Timeline

### âœ… Week 1â€“2: System Architecture & Data Pipeline (COMPLETED)
- [x] Define high-level module layouts
- [x] Implement document loader interface (PDF, MD, text)
- [x] Implement text chunker and tokenizer
- [x] Design and test `rrl ingest` with CLI parsing
- [x] Validate functionality and performance

### âœ… Week 3â€“4: Embedding Engine (COMPLETED)
- [x] Implement `Embedder` trait abstraction
- [x] Integrate `tch` and `onnxruntime` backends
- [x] Add `rrl embed` for local embedding
- [x] Support pooling strategies (mean, CLS) and normalization
- [x] Create persistent embedding cache (SQLite)
- [x] Benchmark embedding throughput and GPU utilization

### âœ… Week 5â€“6: Indexing & Retrieval (COMPLETED)
- [x] Integrate HNSW and Tantivy for dense/sparse retrieval
- [x] Implement `Retriever` trait and hybrid retriever
- [x] Add `rrl query` and evaluation commands
- [x] Measure Recall@k and MRR metrics
- [x] Optimize query latency with multithreading

### âœ… Week 7â€“8: Fine-Tuning & Evaluation Framework (COMPLETED)
- [x] Implement LoRA/QLoRA/DoRA fine-tuning using Candle
- [x] Add grounding-aware loss function
- [x] Build `rrl train` and `rrl eval` commands
- [x] Integrate generation metrics (F1, EM, ROUGE-L, Perplexity)
- [x] Multi-adapter support

### âœ… Week 8.5: Web Interface & API (COMPLETED)
- [x] FastAPI backend with REST API
- [x] React frontend with 7 pages
- [x] Live training dashboard with WebSocket
- [x] RAG workflow interface
- [x] Model browser and evaluation dashboard
- [x] Data upload with drag-and-drop

### âœ… Week 9: RAG Pipeline & LLM Integration (COMPLETED)
- [x] Implement `rrl rag` command with full RAG pipeline
- [x] Integrate decoder models: **Qwen2**, **LLaMA**, **Mistral**
- [x] Web-based dashboard (React UI - primary interface)
- [x] Transitioned from Terminal UI (ratatui) to Web UI
- [x] Support for fine-tuned checkpoints in RAG pipeline

### âœ… Week 10: Final Integration & Documentation (COMPLETED)
- [x] Complete full end-to-end RAG workflow
- [x] MS MARCO evaluation support
- [x] Multi-architecture model loading (encoder + decoder)
- [x] Comprehensive CLI with all commands
- [x] Documentation and README updates

---

## ğŸ‘¥ Team Roles

| Team Member | Responsibilities |
|-------------|------------------|
| **Kevin Lin** | Backend Systems & Embedding/Retrieval: Core framework architecture, data loaders, embedding engine, retrieval (HNSW + Tantivy), hybrid retriever design, performance optimization |
| **Liz Zhu** | Training, Evaluation & Serving: LoRA fine-tuning pipeline, evaluation metrics, web interface, API server implementation, dashboard, Docker packaging |

---

## ğŸ§ª Testing

```bash
# Run all tests
cargo test

# Run specific test
cargo test test_bert_lora

# Run with output
cargo test -- --nocapture

# Test frontend
cd ui && npm test
```

---

## ğŸ“š Documentation

- **[CODE_STANDARDS.md](CODE_STANDARDS.md)** â€” Code formatting, structure, and testing guidelines
- **[TASK_MANAGEMENT.md](TASK_MANAGEMENT.md)** â€” Development workflow and task management
- **[Proposal.md](Proposal.md)** â€” Original project proposal

---

## ğŸ¤ Contributing

We welcome contributions! Please see our development workflow:

1. Read [CODE_STANDARDS.md](CODE_STANDARDS.md) and [TASK_MANAGEMENT.md](TASK_MANAGEMENT.md)
2. Create a feature branch
3. Make changes following code standards
4. Write tests
5. Format code: `cargo fmt && black . && npm run format`
6. Run tests: `cargo test`
7. Submit pull request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Candle** â€” Rust ML framework by HuggingFace
- **HuggingFace** â€” Model hub and transformers
- **FastAPI** â€” Python web framework
- **React** â€” UI framework
- **TailwindCSS** â€” Styling framework
- **hnsw_rs** â€” HNSW implementation
- **tantivy** â€” Full-text search engine

---

## âš¡ Quick Links

- **GitHub:** https://github.com/kevinlin29/ECE1724
- **Web UI:** http://localhost:5173
- **API Docs:** http://localhost:8000/docs
- **Original Proposal:** [Proposal.md](Proposal.md)

---